@article{Ashraf2009,
abstract = {Pain is typically assessed by patient self-report. Self-reported pain, however, is difficult to interpret and may be impaired or in some circumstances (i.e., young children and the severely ill) not even possible. To circumvent these problems behavioral scientists have identified reliable and valid facial indicators of pain. Hitherto, these methods have required manual measurement by highly skilled human observers. In this paper we explore an approach for automatically recognizing acute pain without the need for human observers. Specifically, our study was restricted to automatically detecting pain in adult patients with rotator cuff injuries. The system employed video input of the patients as they moved their affected and unaffected shoulder. Two types of ground truth were considered. Sequence-level ground truth consisted of Likert-type ratings by skilled observers. Frame-level ground truth was calculated from presence/absence and intensity of facial actions previously associated with pain. Active appearance models (AAM) were used to decouple shape and appearance in the digitized face images. Support vector machines (SVM) were compared for several representations from the AAM and of ground truth of varying granularity. We explored two questions pertinent to the construction, design and development of automatic pain detection systems. First, at what level (i.e., sequence- or frame-level) should datasets be labeled in order to obtain satisfactory automatic pain detection performance? Second, how important is it, at both levels of labeling, that we non-rigidly register the face? © 2009 Elsevier B.V. All rights reserved.},
author = {Ashraf, Ahmed Bilal and Lucey, Simon and Cohn, Jeffrey F. and Chen, Tsuhan and Ambadar, Zara and Prkachin, Kenneth M. and Solomon, Patricia E.},
doi = {10.1016/j.imavis.2009.05.007},
file = {:C$\backslash$:/Users/Kasper/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ashraf et al. - 2009 - The painful face - Pain expression recognition using active appearance models.pdf:pdf},
isbn = {9781595938176},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Active appearance models,Automatic facial image analysis,FACS,Facial expression,Pain,Support vector machines},
number = {12},
pages = {1788--1796},
pmid = {22837587},
publisher = {Elsevier B.V.},
title = {{The painful face - Pain expression recognition using active appearance models}},
url = {http://dx.doi.org/10.1016/j.imavis.2009.05.007},
volume = {27},
year = {2009}
}
@inproceedings{Asthana11posenormalization,
author = {Asthana, Akshay and Jones, Michael J and Marks, Tim K},
booktitle = {In BMVC},
title = {{Pose normalization via learned 2d warping for fully automatic face recognition}},
year = {2011}
}
@article{CC01a,
annote = {Software available at $\backslash$url\{http://www.csie.ntu.edu.tw/\~{}cjlin/libsvm\}},
author = {Chang, Chih-Chung and Lin, Chih-Jen},
journal = {ACM Transactions on Intelligent Systems and Technology},
number = {3},
pages = {27:1----27:27},
title = {{\{LIBSVM\}: A library for support vector machines}},
volume = {2},
year = {2011}
}
@book{Ekman1978,
abstract = {Ekman, P.; Friesen, WV.; Hager, JC. The facial action coding system. Salt Lake City, UT: Research Nexus eBook; 2002.},
author = {Ekman, Paul and Friesen, Wallance V.},
booktitle = {Consulting},
isbn = {0931835011},
title = {{The Facial Action Coding System.}},
year = {1978}
}
@article{florealearning,
author = {Florea, Corneliu and Florea, Laura and Vertan, Constantin},
title = {{Learning Pain from Emotion: Transferred HoT Data Representation for Pain Intensity Estimation}}
}
@article{jia2014caffe,
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
journal = {arXiv preprint arXiv:1408.5093},
title = {{Caffe: Convolutional Architecture for Fast Feature Embedding}},
year = {2014}
}
@article{Kaltwang2012,
abstract = {Automatic pain recognition is an evolving research area with promising applications in health care. In this paper, we propose the first fully automatic approach to continuous pain intensity estimation from facial images. We first learn a set of independent regression functions for continuous pain intensity estimation using different shape (facial landmarks) and appearance (DCT and LBP) features, and then perform their late fusion. We show on the recently published UNBC-MacMaster Shoulder Pain Expression Archive Database that late fusion of the afore-mentioned features leads to better pain intensity estimation compared to feature-specific pain intensity estimation.},
author = {Kaltwang, Sebastian and Rudovic, Ognjen and Pantic, Maja},
doi = {10.1007/978-3-642-33191-6\_36},
file = {:C$\backslash$:/Users/Kasper/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaltwang, Rudovic, Pantic - 2012 - Continuous pain intensity estimation from facial expressions.pdf:pdf},
isbn = {9783642331909},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {368--377},
title = {{Continuous pain intensity estimation from facial expressions}},
volume = {7432 LNCS},
year = {2012}
}
@inproceedings{Kanade2000,
abstract = {Within the past decade, significant effort has occurred in developing methods of facial expression analysis. Because most investigators have used relatively limited data sets, the generalizability of these various methods remains unknown. We describe the problem space for facial expression analysis, which includes level of description, transitions among expressions, eliciting conditions, reliability and validity of training and test data, individual differences in subjects, head orientation and scene complexity image characteristics, and relation to non-verbal behavior. We then present the CMU-Pittsburgh AU-Coded Face Expression Image Database, which currently includes 2105 digitized image sequences from 182 adult subjects of varying ethnicity, performing multiple tokens of most primary FACS action units. This database is the most comprehensive testbed to date for comparative studies of facial expression analysis},
author = {Kanade, T and Cohn, J.F.},
booktitle = {Proceedings of the 4th IEEE International Conference on Automatic Face and Gesture Recognition},
doi = {10.1109/AFGR.2000.840611},
isbn = {0-7695-0580-5},
keywords = {CMU-Pittsburgh AU-Coded Face Expression Image Data,FACS action units,Facial features,Gold,Head,Image analysis,Image databases,Layout,Prototypes,Robots,Testing,description level,digitized image sequences,eliciting conditions,expression transitions,face recognition,facial expression analysis,head orientation,image characteristics,image sequences,non-verbal behavior,reliability,scene complexity,subject differences,validity,visual databases},
pages = {46--53},
title = {{Comprehensive database for facial expression analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=840611},
year = {2000}
}
@article{DBLP:journals/corr/KarayevHWAD13,
author = {Karayev, Sergey and Hertzmann, Aaron and Winnemoeller, Holger and Agarwala, Aseem and Darrell, Trevor},
journal = {CoRR},
title = {{Recognizing Image Style}},
url = {http://arxiv.org/abs/1311.3715},
volume = {abs/1311.3715},
year = {2013}
}
@article{mnistlecun,
author = {Lecun, Yann and Cortes, Corinna},
keywords = { mnist, recognition,ai},
title = {{The MNIST database of handwritten digits}},
url = {http://yann.lecun.com/exdb/mnist/}
}
@article{Lenc2014,
author = {Lenc, A. Vedaldi and K.},
journal = {CoRR},
title = {{MatConvNet - Convolutional Neural Networks for MATLAB}},
volume = {1412.4564},
year = {2014}
}
@book{LISAlab2015,
author = {LISA lab, University of Montreal},
title = {{Deep Learning Tutorial, Release 0.1}},
year = {2015}
}
@inproceedings{Lucey2012,
abstract = {In intensive care units in hospitals, it has been recently shown that enormous improvements in patient outcomes can be gained from the medical staff periodically monitoring patient pain levels. However, due to the burden/stress that the staff are already under, this type of monitoring has been difficult to sustain so an automatic solution could be an ideal remedy. Using an automatic facial expression system to do this represents an achievable pursuit as pain can be described via a number of facial action units (AUs). To facilitate this work, the "University of Northern British Columbia-McMaster Shoulder Pain Expression Archive Database" was collected which contains video of participant's faces (who were suffering from shoulder pain) while they were performing a series of range-of-motion tests. Each frame of this data was AU coded by certified FACS coders, and self-report and observer measures at the sequence level were taken as well. To promote and facilitate research into pain and augmentcurrent datasets, we have publicly made available a portion of this database, which includes 200 sequences across 25 subjects, containing more than 48,000 coded frames of spontaneous facial expressions with 66-point AAM tracked facial feature landmarks. In addition to describing the data distribution, we give baseline pain and AU detection results on a frame-by-frame basis at the binary-level (i.e. AU vs. no-AU and pain vs. no-pain) using our AAM/SVM system. Another contribution we make is classifying pain intensities at the sequence-level by using facial expressions and 3D head pose changes. © 2011 Elsevier B.V. All rights reserved.},
author = {Lucey, Patrick and Cohn, Jeffrey F. and Prkachin, Kenneth M. and Solomon, Patricia E. and Chew, Sien and Matthews, Iain},
booktitle = {Image and Vision Computing},
doi = {10.1016/j.imavis.2011.12.003},
isbn = {0262-8856},
issn = {02628856},
keywords = {Action Units (AUs),Active Appearance Models (AAMs),FACS,Pain},
number = {3},
pages = {197--205},
title = {{Painful monitoring: Automatic pain monitoring using the UNBC-McMaster shoulder pain expression archive database}},
volume = {30},
year = {2012}
}
@inproceedings{Lucey2011,
abstract = {A major factor hindering the deployment of a fully functional automatic facial expression detection system is the lack of representative data. A solution to this is to narrow the context of the target application, so enough data is available to build robust models so high performance can be gained. Automatic pain detection from a patient's face represents one such application. To facilitate this work, researchers at McMaster University and University of Northern British Columbia captured video of participant's faces (who were suffering from shoulder pain) while they were performing a series of active and passive range-of-motion tests to their affected and unaffected limbs on two separate occasions. Each frame of this data was AU coded by certified FACS coders, and self-report and observer measures at the sequence level were taken as well. This database is called the UNBC-McMaster Shoulder Pain Expression Archive Database. To promote and facilitate research into pain and augment current datasets, we have publicly made available a portion of this database which includes: 1) 200 video sequences containing spontaneous facial expressions, 2) 48,398 FACS coded frames, 3) associated pain frame-by-frame scores and sequence-level self-report and observer measures, and 4) 66-point AAM landmarks. This paper documents this data distribution in addition to describing baseline results of our AAM/SVM system. This data will be available for distribution in March 2011.},
author = {Lucey, Patrick and Cohn, Jeffrey F. and Prkachin, Kenneth M. and Solomon, Patricia E. and Matthews, Iain},
booktitle = {2011 IEEE International Conference on Automatic Face and Gesture Recognition and Workshops, FG 2011},
doi = {10.1109/FG.2011.5771462},
isbn = {9781424491407},
pages = {57--64},
title = {{Painful data: The UNBC-McMaster shoulder pain expression archive database}},
year = {2011}
}
@article{ufldl,
author = {Ng, Andrew and Ngiam, Jiquan and Foo, Chuan Y and Mai, Yifan and Suen, Caroline},
journal = {http://ufldl.stanford.edu/wiki/index.php/UFLDL\_Tutorial},
title = {{UFLDL Tutorial}},
year = {2010}
}
@mastersthesis{IMM2012-06284,
author = {Palm, R B},
title = {{Prediction as a candidate for learning deep hierarchical models of data}},
year = {2012}
}
@article{Prkachin1992,
abstract = {A number of facial actions have been found to be associated with pain. However, the consistency with which these actions occur during pain of different types has not been examined. This paper focuses on the consistency of facial expressions during pain induced by several modalities of nociceptive stimulation. Forty-one subjects were exposed to pain induced by electric shock, cold, pressure and ischemia. Facial actions during painful and pain-free periods were measured with the Facial Action Coding System. Four actions showed evidence of a consistent association with pain, increasing in likelihood, intensity or duration across all modalities; brow lowering, tightening and closing of the eye lids and nose wrinkling/upper lip raising. Factor analyses suggested that the facial actions reflected a general factor with a reasonably consistent pattern across modalities which could be combined into a sensitive single measure of pain expression. The findings suggest that the 4 actions identified carry the bulk of facial information about pain. They also provide evidence for the existence of a universal facial expression of pain. Implications of the findings for the measurement of pain expression are discussed.},
author = {Prkachin, K. M.},
doi = {10.1016/0304-3959(92)90213-U},
file = {:C$\backslash$:/Users/Kasper/Documents/GitHub/TIENPRAU/Litterature/The consistency of facial expressions of pain - a comparison across modalities.pdf:pdf},
isbn = {0304-3959 (Print)$\backslash$n0304-3959 (Linking)},
issn = {03043959},
journal = {Pain},
keywords = {Facial Action Coding System,Facial expression,Nociceptive stimulation,Pain},
pages = {297--306},
pmid = {1491857},
title = {{The consistency of facial expressions of pain: A comparison across modalities}},
volume = {51},
year = {1992}
}
@article{Prkachin2008a,
abstract = {The present study examined psychometric properties of facial expressions of pain. A diverse sample of 129 people suffering from shoulder pain underwent a battery of active and passive range-of-motion tests to their affected and unaffected limbs. The same tests were repeated on a second occasion. Participants rated the maximum pain induced by each test on three self-report scales. Facial actions were measured with the Facial Action Coding System. Several facial actions discriminated painful from non-painful movements; however, brow-lowering, orbit tightening, levator contraction and eye closing appeared to constitute a distinct, unitary action. An index of pain expression based on these actions demonstrated test-retest reliability and concurrent validity with self-reports of pain. The findings support the concept of a core pain expression with desirable psychometric properties. They are also consistent with the suggestion of individual differences in pain expressiveness. Reasons for varying reports of relations between pain expression and self-reports in previous studies are discussed. ?? 2008 International Association for the Study of Pain.},
author = {Prkachin, Kenneth M. and Solomon, Patricia E.},
doi = {10.1016/j.pain.2008.04.010},
file = {:C$\backslash$:/Users/Kasper/Documents/GitHub/TIENPRAU/Litterature/The structure, reliability and validity of pain expression Evidence from patients with shoulder pain.pdf:pdf},
isbn = {1872-6623 (Electronic) 0304-3959 (Linking)},
issn = {03043959},
journal = {Pain},
keywords = {Facial Action Coding System,Facial expression,Pain,Pain expression,Shoulder pain},
number = {2},
pages = {267--274},
pmid = {18502049},
publisher = {International Association for the Study of Pain},
title = {{The structure, reliability and validity of pain expression: Evidence from patients with shoulder pain}},
url = {http://dx.doi.org/10.1016/j.pain.2008.04.010},
volume = {139},
year = {2008}
}
@inproceedings{Rudovic2011,
author = {Rudovic, Ognjen and Pantic, Maja},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126407},
isbn = {9781457711015},
issn = {1550-5499},
pages = {1495--1502},
title = {{Shape-constrained Gaussian process regression for facial-point-based head-pose normalization}},
year = {2011}
}
@inproceedings{Shen2012a,
abstract = {Visual attention is the ability to select visual stimuli that are most behaviorally relevant among the many others. It allows us to allocate our limited processing resources to the most informative part of the visual scene. In this paper, we learn general high-level concepts with the aid of selective attention in a principled un- supervised framework, where a three layer deep network is built and greedy layer- wise training is applied to learn mid- and high- level features from salient regions of images. The network is demonstrated to be able to successfully learn mean- ingful high-level concepts such as faces and texts in the third-layer and mid-level features like junctions, textures, and parallelism in the second-layer. Unlike pre- trained object detectors that are recently included in saliency models to predict semantic objects, the higher-level features we learned are general base features that are not restricted to one or few object categories. A saliency model built upon the learned features demonstrates its competitive predictive power in natural scenes compared with existing methods.},
author = {Shen, Chengyao and Song, Mingli and Qi, Zhao},
booktitle = {NIPS 2012 Neural Information Processing Systems},
number = {65},
title = {{Learning High-Level Concepts by Training A Deep Network on Eye Fixations}},
year = {2012}
}
@article{Viola2001,
abstract = { This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "integral image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
author = {Viola, P. and Jones, M.},
doi = {10.1109/CVPR.2001.990517},
isbn = {0-7695-1272-0},
issn = {1063-6919},
journal = {Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001},
pmid = {7143246},
title = {{Rapid object detection using a boosted cascade of simple features}},
volume = {1},
year = {2001}
}
@inproceedings{Deng09imagenet:a,
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-jia and Li, Kai and Fei-fei, Li},
booktitle = {In CVPR},
title = {{Imagenet: A large-scale hierarchical image database}},
year = {2009}
}
@techreport{Krizhevsky09cifar,
author = {Krizhevsky, Alex},
title = {{Learning multiple layers of features from tiny images}},
year = {2009}
}
@misc{Corporation2015,
author = {Corporation, NVIDIA},
title = {{CUDA Toolkit}},
url = {https://developer.nvidia.com/cuda-toolkit},
year = {2015}
}
